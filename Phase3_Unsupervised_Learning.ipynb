# -------------------------------------------------------------------
# --- SWE485 - Phase 3: Unsupervised Learning (Fixed Notebook) ---
# -------------------------------------------------------------------
#
# This notebook fulfills all requirements for Phase 3, focusing on 
# clustering the 'customer_booking.csv' dataset to find customer segments.
#

# --- 1. SETUP & LIBRARIES ---
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from IPython.display import display, Markdown

# Ignore warnings for cleaner output
warnings.filterwarnings('ignore')
sns.set(style='whitegrid')
print("--- [1] Libraries imported successfully. ---")


# --- 2. DATA LOADING & PREPARATION ---
# We will use the 'customer_booking.csv' dataset as identified in Phase 1
# for this unsupervised task.

try:
    # Path is relative to the /Unsupervised_Learning folder
    file_path = '../Dataset/customer_booking.csv' 
    df = pd.read_csv(file_path, encoding='latin1')
    print(f"Loaded data from: {file_path}")
except FileNotFoundError:
    try:
        print(f"Warning: Could not find '{file_path}'. Trying local 'customer_booking.csv'.")
        df = pd.read_csv('customer_booking.csv', encoding='latin1')
        print("Loaded data from local 'customer_booking.csv'.")
    except FileNotFoundError as e:
        raise FileNotFoundError("Could not find 'customer_booking.csv' in the expected paths.") from e

# --- HINT: Remove the class label ---
# Per the project instructions, we drop the supervised target 'booking_complete'
if 'booking_complete' in df.columns:
    df_cluster = df.drop('booking_complete', axis=1)
    print("Dropped 'booking_complete' label as required for clustering.")
else:
    df_cluster = df.copy()

# --- Define Feature Types ---
# Based on Phase 1 EDA, we select features relevant for customer segmentation.
# We will drop 'route' and 'booking_origin' as they are high-cardinality text.

numeric_features = [
    'num_passengers', 
    'purchase_lead', 
    'length_of_stay', 
    'flight_hour',
    'flight_duration'
]

categorical_features = [
    'sales_channel', 
    'trip_type',
    'flight_day'
]

# Binary features will be treated as numeric (imputed with 0 and scaled)
binary_features = [
    'wants_extra_baggage', 
    'wants_preferred_seat', 
    'wants_in_flight_meals'
]

# Ensure we only use columns that actually exist in the dataframe to avoid KeyErrors
requested_features = numeric_features + categorical_features + binary_features
existing_features = [f for f in requested_features if f in df_cluster.columns]

missing = set(requested_features) - set(existing_features)
if missing:
    print(f"Warning: The following expected features are missing from the dataset and will be skipped: {sorted(missing)}")

# Split back the lists to only include existing ones:
numeric_features = [f for f in numeric_features if f in df_cluster.columns]
categorical_features = [f for f in categorical_features if f in df_cluster.columns]
binary_features = [f for f in binary_features if f in df_cluster.columns]
features_to_use = numeric_features + categorical_features + binary_features

if not features_to_use:
    raise ValueError("No valid features found in the dataframe. Check column names in 'customer_booking.csv'.")

print(f"Using features: {features_to_use}")


# --- Preprocessing Pipeline ---
# This handles missing values and scaling/encoding in one step.

# Pipeline for numeric (and binary) features
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), # Use median for skewed data
    ('scaler', StandardScaler())
])

# Pipeline for categorical features
# We request a dense one-hot encoding to avoid sparse matrix issues downstream.
# Some sklearn versions use parameter name 'sparse' or 'sparse_output'; using 'sparse=False'
# is widely supported, but if your sklearn version warns, it's still okay.
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

# Create the full preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features + binary_features if (numeric_features + binary_features) else []),
        ('cat', categorical_transformer, categorical_features if categorical_features else [])
    ],
    remainder='drop' 
)

# Apply the preprocessing
X_prepared = preprocessor.fit_transform(df_cluster[features_to_use])

# Ensure dense array (KMeans, silhouette_score, PCA expect dense arrays)
if hasattr(X_prepared, "toarray"):
    try:
        X_prepared = X_prepared.toarray()
    except Exception:
        # fallback: convert via numpy.array
        import numpy as _np
        X_prepared = _np.array(X_prepared)

print(f"--- [2] Data loaded & prepared. Final shape for clustering: {X_prepared.shape} ---")


# --- 3. ALGORITHM APPLICATION & JUSTIFICATION ---
display(Markdown("""
## 1. Algorithm Application & Justification

We have selected **K-Means Clustering** as our unsupervised learning algorithm.

**Justification:**
1.  **Project Goal:** K-Means is ideal for partitioning data into distinct groups, which directly aligns with our goal of identifying "traveler personas" or customer segments from the booking data.
2.  **Efficiency:** It is computationally efficient and scales well to our dataset (suitable for tens of thousands of rows).
3.  **Interpretability:** The resulting cluster centroids are highly interpretable. We can analyze the average values of features (like `purchase_lead` or `num_passengers`) for each cluster to understand its characteristics.
4.  **Metric Alignment:** The K-Means 'inertia_' attribute is the **Total Within-Cluster Sum of Squares (WCSS)**, a metric explicitly required by the project PDF for evaluation.
"""))


# --- 4. EVALUATION & VISUALIZATION ---
display(Markdown("## 2. Evaluation & Visualization"))
print("\n--- [3] Finding Optimal K (Elbow Method)... ---")

# --- Metric 1: Within-Cluster-Sum-of-Squares (WCSS) ---
# We use the Elbow Method to find a good value for 'k'
inertia_values = []
k_range = range(1, 11) # Test K from 1 to 10

for k in k_range:
    kmeans_test = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
    kmeans_test.fit(X_prepared)
    inertia_values.append(kmeans_test.inertia_) # This is the WCSS

# Plot the Elbow Method
plt.figure(figsize=(10, 6))
plt.plot(k_range, inertia_values, marker='o', linestyle='--')
plt.title('Elbow Method (WCSS) for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (Inertia)')
plt.xticks(k_range)
plt.grid(True)
plt.show()

display(Markdown("""
### 2.1. Analysis of Elbow Method (WCSS)
The plot above shows the WCSS (Inertia) for K=1 to 10. We are looking for the "elbow," the point where the rate of decrease sharply flattens. 
Based on the chart, choose an appropriate K (the original notebook selected **K=4**). We will proceed with `K=4` to stay consistent with that choice.
"""))

# --- Train Final Model & Metric 2: Silhouette Score ---
print("\n--- [4] Training Final Model & Calculating Silhouette Score... ---")
OPTIMAL_K = 4 

kmeans_final = KMeans(n_clusters=OPTIMAL_K, init='k-means++', n_init=10, random_state=42)
kmeans_final.fit(X_prepared)

# Get the cluster assignments
cluster_labels = kmeans_final.labels_

# Calculate Silhouette Score
# We use a sample for speed if the dataset is large
if X_prepared.shape[0] > 10000:
    print("Calculating Silhouette Score on a sample of 10k rows for speed...")
    from sklearn.utils import resample
    # resample both X and labels in parallel (no stratify parameter here)
    X_sample, labels_sample = resample(X_prepared, cluster_labels, n_samples=10000, random_state=42)
    silhouette_avg = silhouette_score(X_sample, labels_sample)
else:
    silhouette_avg = silhouette_score(X_prepared, cluster_labels)

display(Markdown(f"""
### 2.2. Evaluation Metric: Silhouette Score
To validate our choice of K={OPTIMAL_K}, we calculated the **Silhouette Coefficient**.
- **Silhouette Score: {silhouette_avg:.3f}**

A score greater than 0 indicates that data points are, on average, closer to their own cluster's center than to other cluster centers. A positive score suggests that the clusters are reasonably well-separated and meaningful.
"""))

# --- Visualization: 2D Cluster Plot (using PCA) ---
print("\n--- [5] Generating 2D Cluster Visualization (using PCA)... ---")
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_prepared)

# Create a new DataFrame for plotting
df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
df_pca['cluster'] = cluster_labels

# Plot the clusters
plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='PC1', 
    y='PC2', 
    hue='cluster', 
    data=df_pca, 
    palette='viridis', 
    s=50, 
    alpha=0.7
)
plt.title(f'Customer Segments (K={OPTIMAL_K}) - Visualized with PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()


# --- 5. INTEGRATION & INSIGHT ---
display(Markdown("## 3. Integration & Insight"))
print("\n--- [6] Analyzing Cluster Profiles for Personas... ---")

# Add the cluster labels back to the original dataframe for analysis
df['cluster'] = cluster_labels

# Analyze the clusters by looking at the average values
available_numeric_and_binary = [f for f in (numeric_features + binary_features) if f in df.columns]
if available_numeric_and_binary:
    cluster_analysis_mean = df.groupby('cluster')[available_numeric_and_binary].mean()
    display(Markdown("### 3.1. Cluster Profile Analysis (Mean Values)"))
    display(cluster_analysis_mean.style.format("{:.2f}"))
else:
    print("No numeric/binary features available for mean analysis.")

# Analyze categorical features
display(Markdown("### 3.1. Cluster Profile Analysis (Categorical Modes)"))
for col in categorical_features:
    if col in df.columns:
        display(Markdown(f"**{col}**"))
        # Show the most common value per cluster
        top_values = df.groupby('cluster')[col].agg(lambda x: x.mode().iat[0] if not x.mode().empty else pd.NA).to_frame(name='most_common')
        display(top_values)
    else:
        print(f"Column '{col}' not found in dataframe; skipping categorical analysis for it.")


# --- FINAL REPORT SECTION ---
display(Markdown(f"""
### 3.2. Integration & Insight (Final Report)

This unsupervised clustering has successfully segmented our customers into {OPTIMAL_K} distinct groups. By analyzing their average behaviors, we can create 'personas' for each.

*(Note: The analysis below is an **example** based on typical results. You **must edit this** using the tables generated above.)*

* **Cluster 0: "The Planners"**
    * **Analysis:** This group has a very **high `purchase_lead`** (e.g., ~150 days) and an average `length_of_stay`. They are also more likely to use the `Internet` sales channel.
    * **Insight:** These are organized customers who book far in advance.

* **Cluster 1: "The Families / Group Travelers"**
    * **Analysis:** This group has the **highest `num_passengers`** (e.g., > 2.0) and also the highest rates of `wants_extra_baggage` and `wants_in_flight_meals`.
    * **Insight:** This segment represents families or groups who value and purchase ancillary services.

* **Cluster 2: "The Last-Minute Business/Solo Travelers"**
    * **Analysis:** This group has a **very low `purchase_lead`** (e.g., ~20 days), `num_passengers` is low (e.g., ~1.0), and `length_of_stay` is short. They are also more likely to use the `Mobile` sales channel.
    * **Insight:** These are spontaneous solo travelers, likely for business or short trips.

* **Cluster 3: "The Comfort-Seekers"**
    * **Analysis:** This group has an average `purchase_lead` and `num_passengers`, but has the **highest rate of `wants_preferred_seat`** and a longer `flight_duration`.
    * **Insight:** This group prioritizes comfort, especially on long-haul flights, and is willing to pay for seat upgrades.

---
### **How Clusters Improve the Supervised Model (Integration)**

Our Phase 2 model was a **supervised intent classifier** (on `conversation2.csv`). These customer segments (from `customer_booking.csv`) can directly enhance that advice system:

1.  **Feature Engineering:** The cluster label (`cluster`) is a new, powerful feature. If a user is logged in, we can fetch their cluster (based on past bookings) and feed it to the Phase 2 model. The model can learn that `intent: book_hotel` + `cluster: "Families"` requires a different response than `intent: book_hotel` + `cluster: "Solo"`.

2.  **Personalized Advice (The "Why"):** This segmentation allows our advice system to be proactive and intelligent:
    * If the model identifies the user as **Cluster 1 ("Families")**, it can modify its advice: "I see you're booking for a group. Would you like me to find hotels with family rooms or add extra baggage to your flight?"
    * If the user is **Cluster 2 ("Last-Minute")**, the system can change its tone: "Prices are rising for these dates. I recommend booking now. Do you want to add flight price protection?"
    * If the user is **Cluster 3 ("Comfort-Seekers")**, the advice system can proactively upsell: "I found a flight. Would you like to upgrade to a preferred seat for your 8-hour journey?"

By combining the **unsupervised segments (who the user is)** with the **supervised model (what the user wants)**, we create a far more robust and personalized advice system, fulfilling the project's core goal.
"""))

print("\n--- [7] Phase 3 Complete. ---")
